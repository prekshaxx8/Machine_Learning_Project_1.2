# üåü Supervised Learning Models on Import-Export Dataset üåü

### üìÑ **Project Overview**
This project demonstrates the application of **Supervised Machine Learning Models** to classify and analyze an import-export dataset. By leveraging Python libraries such as **Pandas**, **NumPy**, and popular ML frameworks, the project explores data preprocessing, visualization, model building, and performance evaluation. 

---

## üóÇÔ∏è **Contents**
- [Project Information](#project-information)
- [Description of Data](#description-of-data)
- [Project Objectives](#project-objectives)
- [Exploratory Data Analysis](#exploratory-data-analysis)
- [Machine Learning Models](#machine-learning-models)
- [Performance Metrics](#performance-metrics)
- [Key Observations](#key-observations)

---

## üìå **Project Information**
- **Title:** Supervised Learning Models on Import-Export Dataset
- **Prepared By:** Preksha Verma (Roll No. 055032)
- **Tools Used:** Python (Jupyter Notebook), Scikit-learn, Matplotlib, Seaborn, XGBoost

---

## üìä **Description of Data**
- **Data Source:** [Kaggle - Import-Export Dataset](https://www.kaggle.com/datasets/chakilamvishwas/imports-exports-15000)
- **Key Variables:**
  - **Numerical:** Quantity, Value, Weight
  - **Categorical:** Country, Product, Port, Shipping Method, etc.
  - **Datetime:** Transaction date
- **Sample Size:** 5001 entries extracted from 15,000 available records.

---

## üéØ **Project Objectives**
- Classify the dataset into meaningful clusters or categories using supervised learning models.
- Identify significant features contributing to classification.
- Evaluate and compare the performance of multiple ML models based on key metrics.

---

## üîç **Exploratory Data Analysis**
- **Preprocessing:**
  - No missing values were detected.
  - Data was encoded using **Ordinal Encoder** and scaled using **Min-Max Scaling**.
- **Visualization:**
  - Bar charts, heatmaps, histograms, and correlation matrices were used to explore patterns.
- **Descriptive Statistics:**
  - Measures like Mean, Median, Standard Deviation, and Skewness were calculated.

---

## ü§ñ **Machine Learning Models**
The following supervised learning models were applied and evaluated:
- **Logistic Regression**
- **Support Vector Machine (SVM)**
- **Stochastic Gradient Descent (SGD)**
- **Decision Tree**
- **K-Nearest Neighbors (KNN)**
- **Naive Bayes**
- **Random Forest**
- **XGBoost**

### **Model Performance Metrics**
- **Confusion Matrix:** Used to assess sensitivity, precision, recall, and F1-score.
- **Cross-Validation:** Evaluated model robustness using K-Fold CV.
- **Runtime and Memory:** Compared computational efficiency across models.

---

## üìà **Performance Metrics**
| **Model**           | **Accuracy (Test Set)** | **Mean CV Accuracy** | **Runtime (seconds)** | **Memory Usage (MB)** |
|----------------------|-------------------------|-----------------------|------------------------|-----------------------|
| Logistic Regression  | 0.3158                 | 0.3503                | 0.0418                | 0.34                 |
| SVM                  | 0.3251                 | 0.3537                | 2.0424                | 0.38                 |
| SGD                  | 0.3158                 | 0.3503                | 0.1159                | 0.31                 |
| Decision Tree        | 0.3238                 | 0.3331                | 0.0558                | 0.27                 |
| KNN                  | 0.3200                 | 0.3258                | 0.0011                | 0.38                 |
| Naive Bayes          | 0.3171                 | 0.3171                | 0.0045                | 0.11                 |
| Random Forest        | 0.3331                 | 0.3498                | 2.0525                | 0.54                 |
| XGBoost              | 0.3298                 | 0.3529                | 1.1805                | 1.02                 |

---

## üìù **Key Observations**
1. **Best Accuracy:** Random Forest and XGBoost demonstrated the highest accuracy scores.
2. **Fastest Model:** KNN was the fastest to run (0.0011 seconds), followed by Naive Bayes.
3. **Most Memory Efficient:** Naive Bayes had the lowest memory usage (0.11 MB), while XGBoost consumed the most (1.02 MB).
4. **Balanced Performance:** Logistic Regression and SVM showed consistent performance across metrics.

---

